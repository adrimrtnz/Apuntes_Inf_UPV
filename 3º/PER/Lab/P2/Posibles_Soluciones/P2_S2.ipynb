{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports necesarios\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST con GradientBoosting\n",
    "En esta segunda sesión práctica vamos a entrenar clasificadores basados en GradientBoosting.\n",
    "Claramente estos clasificadores son más costosos de entrenar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml(\"mnist_784\")\n",
    "\n",
    "print(mnist.data.shape)\n",
    "print(mnist.target.shape)\n",
    "\n",
    "data = mnist.data\n",
    "targets = mnist.target \n",
    "\n",
    "targets=targets.to_numpy()\n",
    "targets=np.int8(targets)\n",
    "\n",
    "data=data.to_numpy()\n",
    "data=np.float32(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partición de los datos\n",
    "\n",
    "Vamos a partir los datos en tres conjuntos: training, validation y test. Con un 80%, 10% y 10% respectivamente. \n",
    "\n",
    "Emplearemos el conjunto de training para aprender los parámetros del modelos, el conjunto de validation para escoger los mejores hiperparámetros. Finalmente reportaremos el resultado final sobre el conjunto de test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 1**    \n",
    "\n",
    "Realiza la partición de los datos en las particiones definidas (80%,10%,10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(7000, 784)\n",
      "(7000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Solución\n",
    "\n",
    "x_train, x_test, y_train, y_test  = train_test_split(data, targets, test_size=0.1, random_state=1)\n",
    "\n",
    "part=(7000.0)/(70000.0-7000)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=part, random_state=1) \n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 2**   \n",
    "\n",
    "Prueba un clasificador GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solución\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(x_train,y_train)\n",
    "    \n",
    "ypred = gb.predict(x_test)\n",
    "acc = accuracy_score(y_test, ypred)\n",
    "print(acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podrás comprobar como es un clasificador mucho más lento. \n",
    "\n",
    "### **Ejercicio 3**   \n",
    "\n",
    "Para reducir el coste computacional se propone crear un pipeline donde se reduzca el número de características mediante PCA. En concreto el número de componentes (dimensiones) a las que reducimos con PCA es un hyperparámetro que tendrás que estimar con el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.47585714285714287\n",
      "4\n",
      "0.6548571428571428\n",
      "8\n",
      "0.8685714285714285\n",
      "16\n",
      "0.9151428571428571\n",
      "32\n",
      "0.927\n"
     ]
    }
   ],
   "source": [
    "## Solución\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "for n in [2,4,8,16,32]:\n",
    "    clf = make_pipeline(PCA(n_components=n),GradientBoostingClassifier())\n",
    "    print(n)  \n",
    "    clf.fit(x_train, y_train)\n",
    "    ypred = clf.predict(x_val)\n",
    "    acc = accuracy_score(y_val, ypred)\n",
    "    print(acc)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De entre los diferentes parámeros que tiene el clasificador GradientBoosting de sklearn, cabría destacar:\n",
    "\n",
    "**learning_rate**\n",
    "\n",
    "**n_estimators**\n",
    "\n",
    "**min_samples_split**\n",
    "\n",
    "**max_features** \n",
    "\n",
    "\n",
    "Para más información leer la documentación en sklearn.\n",
    "\n",
    "Alguno de estos parámetros influyen considerablemente en la velocidad de optimización. Por ejemplo **max_features** y **min_samples_split** entre otros.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 4**   \n",
    "\n",
    "Se propone variar alguno de estos parámetros para ver si se obtiene una similar tasa de acierto pero con mejor velocidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 0.9288571428571428\n"
     ]
    }
   ],
   "source": [
    "# Solución en celdas separadas para medir tiempo\n",
    "\n",
    "# Coste original : 3m 30 s\n",
    "# Tasa de acierto: 0.927\n",
    "clf = make_pipeline(PCA(n_components=32),\n",
    "        GradientBoostingClassifier())\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ypred = clf.predict(x_val)\n",
    "acc = accuracy_score(y_val, ypred)\n",
    "print(n,acc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 0.9258571428571428\n"
     ]
    }
   ],
   "source": [
    "# Solución, truco max_features=sqrt(dim) approx.\n",
    "# Coste mejorado : 56 s casi 4x\n",
    "# Tasa de acierto: 0.924 frete a 0.927\n",
    "clf = make_pipeline(PCA(n_components=32),\n",
    "        GradientBoostingClassifier(max_features=6, n_estimators=100, min_samples_split=10))\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ypred = clf.predict(x_val)\n",
    "acc = accuracy_score(y_val, ypred)\n",
    "print(n,acc)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente sklearn propone otro tipo de algoritmo de GradientBoosting que soporta paralelismo con OMP además de otras mejoras computacionales basadas en la discretización de las componentes mediante un histograma: el HistGradientBoostingClassifier. Su tiempo de ejecución es mucho mejor. Además se pueden obtener mejores resultados.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Ejercicio 5**   \n",
    "\n",
    "Pruébalo y compara los tiempos de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajuan/.local/lib/python3.10/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 0.9671428571428572\n"
     ]
    }
   ],
   "source": [
    "# Solución\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "clf = make_pipeline(PCA(n_components=32),\n",
    "        HistGradientBoostingClassifier())\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "ypred = clf.predict(x_val)\n",
    "acc = accuracy_score(y_val, ypred)\n",
    "print(n,acc) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio 6**   \n",
    "\n",
    "Prueba los parámetros del HistGradientBoostingClassifier que mejoren la tasa de acierto. En cualquier caso la selección de estos parámetros debe seguir el protocolo de experimentación antes expuesto. Esto es, escoger el mejor parámetro con datos de validación y reportar resultados con los datos de test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
