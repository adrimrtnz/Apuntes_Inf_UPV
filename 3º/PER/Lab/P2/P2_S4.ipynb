{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sesión 4\n",
    "\n",
    "En esta última sesión se propone realizar un ejercicio libre con el objetivo de mejorar lo máximo posible el acierto en la tarea MNIST. A modo de orientación se entiende por una buena tasa de acierto en esta tarea cuando obtenemos más de un 99% de acierto.\n",
    "\n",
    "Para conseguir este resultado podréis probar pipelines que preprocesen los datos adecuadamente. Probar diferentes clasificadores e incluso combinación de los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports necesarios\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adri_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml(\"mnist_784\")\n",
    "\n",
    "print(mnist.data.shape)\n",
    "print(mnist.target.shape)\n",
    "\n",
    "data = mnist.data\n",
    "targets = mnist.target \n",
    "\n",
    "targets=targets.to_numpy()\n",
    "targets=np.int8(targets)\n",
    "\n",
    "data=data.to_numpy()\n",
    "data=np.float32(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partición de los datos\n",
    "\n",
    "Vamos a partir los datos en tres conjuntos: training, validation y test. Con un 80%, 10% y 10% respectivamente. \n",
    "\n",
    "Emplearemos el conjunto de training para aprender los parámetros del modelos, el conjunto de validation para escoger los mejores hiperparámetros. Finalmente reportaremos el resultado final sobre el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partición de Datos\n",
    "X_train, X_aux, y_train, y_aux = train_test_split(data, targets, test_size=0.2, shuffle=True, random_state=23)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_aux, y_aux, test_size=0.5, shuffle=True, random_state=23)\n",
    "\n",
    "X_tv = np.concatenate((X_train, X_val), axis=0)\n",
    "y_tv = np.concatenate((y_train, y_val), axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio: probar todo lo necesario para obtener un acc > 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para 8 clasificadores: 58.01%\n",
      "Para 16 clasificadores: 65.47%\n",
      "Para 32 clasificadores: 73.09%\n",
      "Para 64 clasificadores: 76.40%\n",
      "Para 128 clasificadores: 75.51%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m num_clf \u001b[39min\u001b[39;00m [\u001b[39m8\u001b[39m,\u001b[39m16\u001b[39m,\u001b[39m32\u001b[39m,\u001b[39m64\u001b[39m,\u001b[39m128\u001b[39m,\u001b[39m256\u001b[39m,\u001b[39m512\u001b[39m]:\n\u001b[0;32m      4\u001b[0m     model \u001b[39m=\u001b[39m AdaBoostClassifier(n_estimators\u001b[39m=\u001b[39mnum_clf, random_state\u001b[39m=\u001b[39mseed)\n\u001b[1;32m----> 5\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m      6\u001b[0m     y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_val)\n\u001b[0;32m      7\u001b[0m     acc \u001b[39m=\u001b[39m accuracy_score(y_pred, y_val)\n",
      "File \u001b[1;32mc:\\Users\\adri_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:162\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    159\u001b[0m sample_weight[zero_weight_mask] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    161\u001b[0m \u001b[39m# Boosting step\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m sample_weight, estimator_weight, estimator_error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_boost(\n\u001b[0;32m    163\u001b[0m     iboost, X, y, sample_weight, random_state\n\u001b[0;32m    164\u001b[0m )\n\u001b[0;32m    166\u001b[0m \u001b[39m# Early termination\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\adri_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:569\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Implement a single boost.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \n\u001b[0;32m    532\u001b[0m \u001b[39mPerform a single boost according to the real multi-class SAMME.R\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[39m    If None then boosting has terminated early.\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSAMME.R\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 569\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_boost_real(iboost, X, y, sample_weight, random_state)\n\u001b[0;32m    571\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[0;32m    572\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_boost_discrete(iboost, X, y, sample_weight, random_state)\n",
      "File \u001b[1;32mc:\\Users\\adri_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:578\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[39;00m\n\u001b[0;32m    576\u001b[0m estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m--> 578\u001b[0m estimator\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    580\u001b[0m y_predict_proba \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    582\u001b[0m \u001b[39mif\u001b[39;00m iboost \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\adri_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\adri_\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Ejercicio obtener acc>99%\n",
    "seed = 23\n",
    "for num_clf in [8,16,32,64,128,256,512]:\n",
    "    model = AdaBoostClassifier(n_estimators=num_clf, random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_pred, y_val)\n",
    "    print(f'Para {num_clf} clasificadores: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para 8 DecisionTreeClassifier con profundidad 2: 50.19%\n",
      "Para 16 DecisionTreeClassifier con profundidad 2: 47.57%\n",
      "Para 32 DecisionTreeClassifier con profundidad 2: 44.67%\n",
      "Para 64 DecisionTreeClassifier con profundidad 2: 43.89%\n",
      "Para 128 DecisionTreeClassifier con profundidad 2: 43.69%\n",
      "Para 256 DecisionTreeClassifier con profundidad 2: 43.80%\n",
      "Para 8 DecisionTreeClassifier con profundidad 4: 72.73%\n",
      "Para 16 DecisionTreeClassifier con profundidad 4: 70.87%\n",
      "Para 32 DecisionTreeClassifier con profundidad 4: 71.84%\n",
      "Para 64 DecisionTreeClassifier con profundidad 4: 71.63%\n",
      "Para 128 DecisionTreeClassifier con profundidad 4: 71.63%\n",
      "Para 256 DecisionTreeClassifier con profundidad 4: 71.87%\n",
      "Para 8 DecisionTreeClassifier con profundidad 8: 90.67%\n",
      "Para 16 DecisionTreeClassifier con profundidad 8: 90.53%\n",
      "Para 32 DecisionTreeClassifier con profundidad 8: 90.73%\n",
      "Para 64 DecisionTreeClassifier con profundidad 8: 90.74%\n",
      "Para 128 DecisionTreeClassifier con profundidad 8: 90.59%\n",
      "Para 256 DecisionTreeClassifier con profundidad 8: 90.79%\n",
      "Para 8 DecisionTreeClassifier con profundidad 16: 94.50%\n",
      "Para 16 DecisionTreeClassifier con profundidad 16: 95.30%\n",
      "Para 32 DecisionTreeClassifier con profundidad 16: 95.87%\n",
      "Para 64 DecisionTreeClassifier con profundidad 16: 95.87%\n",
      "Para 128 DecisionTreeClassifier con profundidad 16: 95.99%\n",
      "Para 256 DecisionTreeClassifier con profundidad 16: 96.09%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "seed = 23\n",
    "\n",
    "for max_depth in [2,4,8,16]:\n",
    "    for n_clf in [8,16,32,64,128,256]:\n",
    "        clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "        model = BaggingClassifier(estimator=clf, n_estimators=n_clf, random_state=seed)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_pred, y_val)\n",
    "        print(f'Para {n_clf} DecisionTreeClassifier con profundidad {max_depth}: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para 8 DecisionTreeClassifier con profundidad 14: 94.74%\n",
      "Para 16 DecisionTreeClassifier con profundidad 14: 94.97%\n",
      "Para 32 DecisionTreeClassifier con profundidad 14: 95.39%\n",
      "Para 64 DecisionTreeClassifier con profundidad 14: 95.59%\n",
      "Para 128 DecisionTreeClassifier con profundidad 14: 95.67%\n",
      "Para 256 DecisionTreeClassifier con profundidad 14: 95.86%\n",
      "Para 8 DecisionTreeClassifier con profundidad 15: 94.49%\n",
      "Para 16 DecisionTreeClassifier con profundidad 15: 95.21%\n",
      "Para 32 DecisionTreeClassifier con profundidad 15: 95.47%\n",
      "Para 64 DecisionTreeClassifier con profundidad 15: 95.63%\n",
      "Para 128 DecisionTreeClassifier con profundidad 15: 95.79%\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [14,15,16,17,18]:\n",
    "    for n_clf in [8,16,32,64,128,256]:\n",
    "        clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "        model = BaggingClassifier(estimator=clf, n_estimators=n_clf, random_state=seed)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_pred, y_val)\n",
    "        print(f'Para {n_clf} DecisionTreeClassifier con profundidad {max_depth}: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=14)\n",
    "model = BaggingClassifier(estimator=clf, n_estimators=256, random_state=seed)\n",
    "model.fit(X_tv, y_tv)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_pred, y_test)\n",
    "print(f'Para {n_clf} DecisionTreeClassifier para test: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para 8 RandomForestClassifier con max_depth=2: 57.93%\n",
      "Para 8 RandomForestClassifier con max_depth=4: 77.27%\n",
      "Para 8 RandomForestClassifier con max_depth=8: 89.51%\n",
      "Para 16 RandomForestClassifier con max_depth=2: 59.60%\n",
      "Para 16 RandomForestClassifier con max_depth=4: 79.79%\n",
      "Para 16 RandomForestClassifier con max_depth=8: 91.09%\n",
      "Para 32 RandomForestClassifier con max_depth=2: 61.46%\n",
      "Para 32 RandomForestClassifier con max_depth=4: 81.31%\n",
      "Para 32 RandomForestClassifier con max_depth=8: 91.91%\n",
      "Para 64 RandomForestClassifier con max_depth=2: 65.06%\n",
      "Para 64 RandomForestClassifier con max_depth=4: 82.71%\n",
      "Para 64 RandomForestClassifier con max_depth=8: 92.66%\n",
      "Para 128 RandomForestClassifier con max_depth=2: 66.10%\n",
      "Para 128 RandomForestClassifier con max_depth=4: 83.76%\n",
      "Para 128 RandomForestClassifier con max_depth=8: 93.09%\n",
      "Para 256 RandomForestClassifier con max_depth=2: 66.40%\n",
      "Para 256 RandomForestClassifier con max_depth=4: 83.43%\n",
      "Para 256 RandomForestClassifier con max_depth=8: 93.03%\n",
      "Para 512 RandomForestClassifier con max_depth=2: 66.33%\n",
      "Para 512 RandomForestClassifier con max_depth=4: 83.34%\n",
      "Para 512 RandomForestClassifier con max_depth=8: 93.01%\n",
      "Para 1024 RandomForestClassifier con max_depth=2: 66.01%\n",
      "Para 1024 RandomForestClassifier con max_depth=4: 82.93%\n",
      "Para 1024 RandomForestClassifier con max_depth=8: 93.03%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for n_clf in [8,16,32,64,128,256,512,1024]:\n",
    "    for md in [2,4,8,10,12,14,16]:\n",
    "        clf = RandomForestClassifier(n_estimators=n_clf, max_depth=md, random_state=seed)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_val)\n",
    "        acc = accuracy_score(y_pred, y_val)\n",
    "        print(f'Para {n_clf} RandomForestClassifier con max_depth={md}: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión para Pipeline(steps=[('pca', PCA(n_components=2)),\n",
      "                ('votingclassifier',\n",
      "                 VotingClassifier(estimators=[('rf',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=23)),\n",
      "                                              ('kn4',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=4)),\n",
      "                                              ('kn5',\n",
      "                                               KNeighborsClassifier(n_jobs=-1)),\n",
      "                                              ('kn6',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=6)),\n",
      "                                              ('rf2',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=24))]))]): 43.87%\n",
      "Precisión para Pipeline(steps=[('pca', PCA(n_components=4)),\n",
      "                ('votingclassifier',\n",
      "                 VotingClassifier(estimators=[('rf',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=23)),\n",
      "                                              ('kn4',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=4)),\n",
      "                                              ('kn5',\n",
      "                                               KNeighborsClassifier(n_jobs=-1)),\n",
      "                                              ('kn6',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=6)),\n",
      "                                              ('rf2',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=24))]))]): 64.04%\n",
      "Precisión para Pipeline(steps=[('pca', PCA(n_components=8)),\n",
      "                ('votingclassifier',\n",
      "                 VotingClassifier(estimators=[('rf',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=23)),\n",
      "                                              ('kn4',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=4)),\n",
      "                                              ('kn5',\n",
      "                                               KNeighborsClassifier(n_jobs=-1)),\n",
      "                                              ('kn6',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=6)),\n",
      "                                              ('rf2',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=24))]))]): 90.50%\n",
      "Precisión para Pipeline(steps=[('pca', PCA(n_components=16)),\n",
      "                ('votingclassifier',\n",
      "                 VotingClassifier(estimators=[('rf',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=23)),\n",
      "                                              ('kn4',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=4)),\n",
      "                                              ('kn5',\n",
      "                                               KNeighborsClassifier(n_jobs=-1)),\n",
      "                                              ('kn6',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=6)),\n",
      "                                              ('rf2',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=24))]))]): 96.59%\n",
      "Precisión para Pipeline(steps=[('pca', PCA(n_components=32)),\n",
      "                ('votingclassifier',\n",
      "                 VotingClassifier(estimators=[('rf',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=23)),\n",
      "                                              ('kn4',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=4)),\n",
      "                                              ('kn5',\n",
      "                                               KNeighborsClassifier(n_jobs=-1)),\n",
      "                                              ('kn6',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=6)),\n",
      "                                              ('rf2',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=24))]))]): 97.53%\n",
      "Precisión para Pipeline(steps=[('pca', PCA(n_components=64)),\n",
      "                ('votingclassifier',\n",
      "                 VotingClassifier(estimators=[('rf',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=23)),\n",
      "                                              ('kn4',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=4)),\n",
      "                                              ('kn5',\n",
      "                                               KNeighborsClassifier(n_jobs=-1)),\n",
      "                                              ('kn6',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=6)),\n",
      "                                              ('rf2',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=24))]))]): 97.64%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "seed = 23\n",
    "\n",
    "for nc in [2,4,8,16,32,64]:\n",
    "        clf1 = RandomForestClassifier(n_estimators=256, max_depth=8, random_state=seed)\n",
    "        clf2 = KNeighborsClassifier(n_neighbors=4, n_jobs=-1)\n",
    "        clf3 = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "        clf4 = KNeighborsClassifier(n_neighbors=6, n_jobs=-1)\n",
    "        clf5 = RandomForestClassifier(n_estimators=256, max_depth=8, random_state=seed+1)\n",
    "\n",
    "        vc = VotingClassifier(\n",
    "                estimators=[('rf', clf1), ('kn4', clf2),('kn5', clf3),('kn6', clf4),('rf2', clf5)],\n",
    "                voting='hard')\n",
    "        \n",
    "        pipe = make_pipeline(\n",
    "                PCA(n_components=nc),\n",
    "                vc\n",
    "        )\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_val)\n",
    "        acc = accuracy_score(y_pred, y_val)\n",
    "        print(f'Precisión para {nc} componentes: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión para Pipeline(steps=[('pca', PCA(n_components=32)),\n",
      "                ('votingclassifier',\n",
      "                 VotingClassifier(estimators=[('rf',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=23)),\n",
      "                                              ('kn4',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=4)),\n",
      "                                              ('kn5',\n",
      "                                               KNeighborsClassifier(n_jobs=-1)),\n",
      "                                              ('kn6',\n",
      "                                               KNeighborsClassifier(n_jobs=-1,\n",
      "                                                                    n_neighbors=6)),\n",
      "                                              ('rf2',\n",
      "                                               RandomForestClassifier(max_depth=8,\n",
      "                                                                      n_estimators=256,\n",
      "                                                                      random_state=24))]))]): 97.71%\n"
     ]
    }
   ],
   "source": [
    "clf1 = RandomForestClassifier(n_estimators=256, max_depth=8, random_state=seed)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=4, n_jobs=-1)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "clf4 = KNeighborsClassifier(n_neighbors=6, n_jobs=-1)\n",
    "clf5 = RandomForestClassifier(n_estimators=256, max_depth=8, random_state=seed+1)\n",
    "\n",
    "vc = VotingClassifier(\n",
    "        estimators=[('rf', clf1), ('kn4', clf2),('kn5', clf3),('kn6', clf4),('rf2', clf5)],\n",
    "        voting='hard')\n",
    "\n",
    "pipe = make_pipeline(\n",
    "        PCA(n_components=32),\n",
    "        vc\n",
    ")\n",
    "\n",
    "pipe.fit(X_tv, y_tv)\n",
    "y_pred = pipe.predict(X_test)\n",
    "acc = accuracy_score(y_pred, y_test)\n",
    "print(f'Precisión para {pipe}: {acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión con 2 componentes: 47.54%\n",
      "Precisión con 4 componentes: 66.56%\n",
      "Precisión con 8 componentes: 90.71%\n",
      "Precisión con 16 componentes: 96.00%\n",
      "Precisión con 32 componentes: 97.01%\n",
      "Precisión con 64 componentes: 97.13%\n",
      "La precisión de Pipeline(steps=[('pca', PCA(n_components=64)),\n",
      "                ('votingclassifier',\n",
      "                 VotingClassifier(estimators=[('hg1',\n",
      "                                               HistGradientBoostingClassifier(max_bins=64,\n",
      "                                                                              max_depth=16,\n",
      "                                                                              min_samples_leaf=32)),\n",
      "                                              ('hg2',\n",
      "                                               HistGradientBoostingClassifier(max_bins=32,\n",
      "                                                                              max_depth=16,\n",
      "                                                                              min_samples_leaf=64)),\n",
      "                                              ('hg3',\n",
      "                                               HistGradientBoostingClassifier(l2_regularization=0.4,\n",
      "                                                                              learning_rate=0.3,\n",
      "                                                                              max_bins=64,\n",
      "                                                                              max_depth=8,\n",
      "                                                                              min_samples_leaf=16)),\n",
      "                                              ('hg4',\n",
      "                                               HistGradientBoostingClassifier(l2_regularization=0.2,\n",
      "                                                                              learning_rate=0.3,\n",
      "                                                                              max_bins=100,\n",
      "                                                                              max_depth=16,\n",
      "                                                                              min_samples_leaf=32)),\n",
      "                                              ('hg5',\n",
      "                                               HistGradientBoostingClassifier(l2_regularization=1,\n",
      "                                                                              learning_rate=0.3,\n",
      "                                                                              max_bins=64,\n",
      "                                                                              max_depth=16,\n",
      "                                                                              min_samples_leaf=256))]))]) para el conjunto de train/val es 97.13%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble  import VotingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "max_acc = -1\n",
    "sol = ''\n",
    "for nc in [2,4,8,16,32,64]:\n",
    "\n",
    "    clf1 = HistGradientBoostingClassifier(max_bins=64, max_depth=16,min_samples_leaf=32)\n",
    "    clf2 = HistGradientBoostingClassifier(max_bins=32, max_depth=16,min_samples_leaf=64)\n",
    "    clf3 = HistGradientBoostingClassifier(l2_regularization=0.4, learning_rate=0.3, max_bins=64, max_depth=8, min_samples_leaf=16)\n",
    "    clf4 = HistGradientBoostingClassifier(l2_regularization=0.2, learning_rate=0.3, max_bins=100, max_depth=16, min_samples_leaf=32)\n",
    "    clf5 = HistGradientBoostingClassifier(l2_regularization=1, learning_rate=0.3, max_bins=64, max_depth=16, min_samples_leaf=256)\n",
    "\n",
    "    kn = make_pipeline(\n",
    "                PCA(n_components=nc),\n",
    "                VotingClassifier([\n",
    "                    ('hg1',clf1),\n",
    "                    ('hg2',clf2),\n",
    "                    ('hg3',clf3),\n",
    "                    ('hg4', clf4),\n",
    "                    ('hg5', clf5),\n",
    "                ])\n",
    "            ) \n",
    "    \n",
    "    kn.fit(X_train, y_train)\n",
    "    y_pred = kn.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "        sol = f'La precisión de {kn} para el conjunto de train/val es {acc:.2%}'\n",
    "    print(f'Precisión con {nc} componentes: {acc:.2%}')\n",
    "\n",
    "print(sol)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    subsample=.75,\n",
    "    colsample_bylevel=.75,\n",
    "    max_depth=8,\n",
    "    random_state=23\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'La precisión es {acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión es 98.37%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble  import VotingClassifier\n",
    "\n",
    "clf1 = make_pipeline(\n",
    "    PCA(n_components=42),\n",
    "    KNeighborsClassifier(n_neighbors=3)\n",
    "    )\n",
    "clf2 = XGBClassifier(\n",
    "    n_estimators=130,\n",
    "    subsample=.8,\n",
    "    colsample_bylevel=.9,\n",
    "    max_depth=8,\n",
    "    random_state=23  \n",
    ")\n",
    "\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('HGB',clf1),('knn3', clf2)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "eclf = eclf.fit(X_tv, y_tv)\n",
    "ypred = eclf.predict(X_test)\n",
    "acc = accuracy_score(y_test, ypred)\n",
    "print(f'La precisión es {acc:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
